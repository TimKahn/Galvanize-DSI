{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: Spark/RDD in Python\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviromnent to run spark from python\n",
    "2. extract RDDs from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Initializing a `SparkSession`\n",
    "\n",
    "IPython / IPython notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkSession` that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"images/spark_driver_etc.png\"/>\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"rdd lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext for RDDs (not DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Creating an RDD (from files)\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from an external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| [`sc.parallelize(array)`]() | Create an RDD from a python array or list |\n",
    "| [`sc.textFile(path)`]() | Create an RDD from a text file |\n",
    "| [`sc.pickleFile(path)`]() | Create an RDD from a pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Creating RDDs from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.parallelize()` : create an RDD from a python array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/toy_data.txt', 'r') as fin:\n",
    "    print fin.read()\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('data/toy_data.txt')\n",
    "\n",
    "# to output the content in python [irl, use collect() with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">`sc.pickeFile()` : from a HDFS pickle file\n",
    "\n",
    "The import will give you an rdd composed of whatever table was stored into that file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data/toy_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.pickleFile('data/toy_data.pkl')\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Creating RDDs from S3\n",
    "\n",
    "These two functions above can perform loading from an s3 repository too ! Effortless.\n",
    "\n",
    "<span style=\"color:red\">Warning: don't .collect() RDDs that are too large, or you'll break the internet !</span>\n",
    "\n",
    "**Note**: in order to do that, you need to have launched jupyter with the `--packages` options for aws and hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# link to the S3 repository\n",
    "link = 's3a://dsi-spark-day/airline_data.csv'\n",
    "\n",
    "# creating an RDD...\n",
    "rdd = sc.textFile(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this repository has several hundred thousand rows, but this was so fast ! right ?\n",
    "\n",
    "Not really, Spark is just **lazy**: it only executes the operations when necessary. For instance, when we call for an Action (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many partitions there are...\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform an RDD into another RDD because RDD are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "\\[[Image Source](http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html)\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "In the sequence below, we will in one sequence:\n",
    "1. read an RDD from a text file\n",
    "2. transform by applying `split`\n",
    "3. transform by filtering\n",
    "4. transform by casting some columns to their corresponding type.\n",
    "5. use an action to output the results\n",
    "\n",
    "Each transformation is a method of an RDD, and returns another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/sales.txt', 'r') as fin:\n",
    "    print fin.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall: Input functions, reading RDDs from files, are functions of the SparkContext.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies split() to each row\n",
    "rdd2 = rdd1.map(lambda rowstr : rowstr.split())\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters rows\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount), int(id)+int(product)))\n",
    "\n",
    "# applies casting_function to rows\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "\n",
    "# shows the result\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's see the canonical way to write that in Python...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)   # <= JUST ADD THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">FROM NOW ON WE'LL RELY ON THESE TWO RDDs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map(func)` : applying a function on every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda x : len(x[0]))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using lambda functions, use **argument unpacking** to provide a more readable transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda (name,number) : len(name))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.flatMap(func)` : applying a function on every row and flattening the resulting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.flatMap(lambda (name,number) : [number, number+2, number+len(name)])\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Row reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering an rdd\n",
    "rddout = rdd_sales.filter(lambda (i, date, store, state, pdt, amnt): (state == 'CA'))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling an rdd\n",
    "rddout = rdd_sales.sample(True, 0.4)\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "rddout = rdd_sales.map(lambda (i, date, store, state, pdt, amnt): state)\\\n",
    "                    .distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Methods with a `<k,v>` paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.values()`: returns the values of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.values()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.keys()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_salesperstate = rdd_sales.map(lambda (i, date, store, state, pdt, amnt): (state,amnt))\n",
    "\n",
    "rdd_salesperstate.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating an adhoc list of managers for each state\n",
    "data_array = [['CA', 'matthew'],\n",
    "              ['OR', 'jorge'],\n",
    "              ['WA','matthew'],\n",
    "              ['TX', 'emilie']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_managers = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_salesperstate.join(rdd_managers).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.reduceByKey(func)`: reduce `v`s by their `k` by applying func (what ?)\n",
    "\n",
    "The `func` here needs to be associative and commutative... can you guess why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey(lambda v1,v2 : v1+v2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupByKey(func)`: reduce `v`s by their `k` by applying func (again ?)\n",
    "\n",
    "This can use any function non-commutative\n",
    "\n",
    "#### ***Use `.reduceByKey()` if you can!!!*** --->\n",
    "https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(iterator):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return total / count\n",
    "\n",
    "rdd.groupByKey()\\\n",
    "    .map(lambda (state, iterator): (state, mean(iterator)))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Sorting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by any function (because why not?)\n",
    "rddout = rdd_names.sortBy(lambda (name,number) : (13-number)**2, ascending=True)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting k,v pairs by key\n",
    "rddout = rdd_names.sortByKey(ascending=False)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (a python object, or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in a RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDD’s elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDD’s elements. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : returning the *full* content of an RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic, cause `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to output the content in python\n",
    "collected = rdd_names.collect()\n",
    "\n",
    "# let's check the type of RDD\n",
    "print(\"type of rdd: {}\".format(type(rdd_names)))\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_collected: {}\".format(type(collected)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : returning (any) n lines of an RDD\n",
    "\n",
    "Returns `n` the rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to output the content in python\n",
    "taken = rdd_names.take(2)\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_taken: {}\".format(type(taken)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : returning the first line of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_names.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Actions that compute some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_names.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_names.values().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_names.values().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: you get that right ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_names.values().stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Let's design chains of transformations together !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain an RDD of the states sorted by their decreasing cumulated sales.\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddout = rdd_sales  ### put your transformations here...\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_sales.map(lambda x: (x[3],x[5]))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Word count\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/input.txt', 'r') as fin:\n",
    "    print fin.read()\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd_text = sc.textFile('data/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "You want to create a table of unique words and their occurences.\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "\n",
    "If you had to draw a workflow of the transformations to apply ?\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddout = rdd_text  # put your transformations here...\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_text.flatMap(lambda str : str.split())\\\n",
    "            .map(lambda word: (word,1))\\\n",
    "            .reduceByKey(lambda v1,v2: v1+v2)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_aapl_raw = sc.textFile('data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(rdd_aapl_raw.count()))\n",
    "\n",
    "rdd_aapl_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddout = rdd_aapl_raw # apply transformation here...\n",
    "\n",
    "rddout.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_aapl_raw.filter(lambda line: not line.startswith(\"Date\"))\\\n",
    ".map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda fields: (float(fields[4]),fields[0]))\\\n",
    ".sortBy(lambda (close, date): close, ascending=False)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD: one use case ? oh I don't know maybe... **MACHINE LEARNING !!!**\n",
    "\n",
    "## 3.1. Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in xrange(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
