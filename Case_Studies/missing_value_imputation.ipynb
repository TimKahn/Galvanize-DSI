{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value imputation\n",
    "\n",
    "## Preprocessing data\n",
    "\n",
    "Real world day often contains missing values.  As part of the preprocessing step one common task is to handle missing values.  This is a summary of the things to consider when preprocessing data\n",
    "\n",
    "Examples and text were borrowed from:\n",
    "\n",
    "   * http://scikit-learn.org/stable/modules/preprocessing.htm\n",
    "   * Chris Fonnesbeck (Bio8366 course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16156712 -0.13302017 -0.04334984 -0.14078586 -0.14159387]\n"
     ]
    }
   ],
   "source": [
    "## create some data\n",
    "X,y = make_classification(n_samples=50, n_features=5)\n",
    "print(X.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "## make a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "## scale using sklearn\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_1 = scaler.transform(X_train)\n",
    "X_test_1 = scaler.transform(X_test)\n",
    "\n",
    "## scale without sklearn (just so you know what is happening)\n",
    "X_train_2 = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test_2 = (X_test - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "\n",
    "## are they equal?\n",
    "print(np.array_equal(X_train_1.mean(axis=0),X_train_2.mean(axis=0)))\n",
    "print(np.array_equal(X_test_1.mean(axis=0),X_test_2.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature encoding\n",
    "\n",
    "Often features are not given as continuous values, but rather as categorical classes. For example, variables may be defined as `[\"male\", \"female\"]`, `[\"Europe\", \"US\", \"Asia\"]`, `[\"Disease A\", \"Disease B\", \"Disease C\"]`. Such features can be efficiently coded as integers, for instance `[\"male\", \"US\", \"Disease B\"]` could be expressed as `[0, 1, 1]`.\n",
    "\n",
    "Unfortunately, an integer representation can not be used directly with estimators in scikit-learn, because these expect *continuous* input, and would therefore interpret the categories as being ordered, which for the above examples, would be inappropriate.\n",
    "\n",
    "One approach is to use a \"one-of-K\" or \"one-hot\" encoding, which is implemented in `OneHotEncoder`. This estimator transforms a categorical feature with `m` possible values into `m` binary features, with only one active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\n",
    "enc.fit(data)\n",
    "enc.transform([[0, 1, 3]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the cardinality of each feature is inferred automatically from the dataset; this can be manually overriden using the `n_values` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing\n",
    "\n",
    "`LabelBinarizer` is a utility class to help create a label indicator matrix from a list of multi-class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 0 1 0]]\n",
      "[1 2 4 6]\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1, 2, 6, 4, 2])\n",
    "print(lb.transform((1,4)))\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple labels per instance, use MultiLabelBinarizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 0 1]]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "print(lb.fit_transform([(1, 2), (3,)]))\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabelEncoder` is a utility class to help normalize labels such that they contain only consecutive values between 0 and `n_classes-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 6]\n",
      "[0 0 1 2]\n",
      "[1 1 2 6]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([1,2,2,6])\n",
    "print(le.classes_)\n",
    "print(le.transform([1, 1, 2, 6]))\n",
    "print(le.inverse_transform([0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Imputation\n",
    "\n",
    "Missing data is a common problem in most real-world scientific datasets. While the best way for dealing with missing data will always be preventing their occurrence in the first place, it usually can't be helped, particularly when data are collected passively or voluntarily, or when data collection and recording is distributed among several people. There are a variety of ways for dealing with missing data, from the very naïve to the very sophisticated, and unfortunately the more common approaches tend to be *ad hoc* and will usually do more harm than good. \n",
    "\n",
    "It turns out that more robust methods for imputation are not as difficult to implement as they first appear to be. Two of the best ones are Bayesian imputation and multiple imputation. In this section, we will use **multiple imputation** to account for missing data in a regression analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a motivating example, we will use a dataset of educational outcomes for children with hearing impairment. Here, we are interested in determining factors that are associated with better or poorer learning outcomes. \n",
    "\n",
    "![hearing aid](hearing_aid.jpg)\n",
    "\n",
    "There is a suite of available predictors, including: \n",
    "\n",
    "* gender (`male`)\n",
    "* number of siblings in the household (`siblings`)\n",
    "* index of family involvement (`family_inv`)\n",
    "* whether the primary household language is not English (`non_english`)\n",
    "* presence of a previous disability (`prev_disab`)\n",
    "* non-white race (`non_white`)\n",
    "* age at the time of testing (in months, `age_test`)\n",
    "* whether hearing loss is not severe (`non_severe_hl`)\n",
    "* whether the subject's mother obtained a high school diploma or better (`mother_hs`)\n",
    "* whether the hearing impairment was identified by 3 months of age (`early_ident`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>family_inv</th>\n",
       "      <th>non_english</th>\n",
       "      <th>prev_disab</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  male  siblings  family_inv  non_english  prev_disab  age_test  \\\n",
       "0     40     0       2.0         2.0        False         NaN        55   \n",
       "1     31     1       0.0         NaN        False         0.0        53   \n",
       "2     83     1       1.0         1.0         True         0.0        52   \n",
       "3     75     0       3.0         NaN        False         0.0        55   \n",
       "5     62     0       0.0         4.0        False         1.0        50   \n",
       "\n",
       "   non_severe_hl  mother_hs  early_ident  non_white  \n",
       "0            1.0        NaN        False      False  \n",
       "1            0.0        0.0        False      False  \n",
       "2            1.0        NaN        False       True  \n",
       "3            0.0        1.0        False      False  \n",
       "5            0.0        NaN        False      False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = pd.read_csv('test_scores.csv', index_col=0)\n",
    "test_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 207 entries, 0 to 224\n",
      "Data columns (total 11 columns):\n",
      "score            207 non-null int64\n",
      "male             207 non-null int64\n",
      "siblings         207 non-null float64\n",
      "family_inv       174 non-null float64\n",
      "non_english      207 non-null bool\n",
      "prev_disab       189 non-null float64\n",
      "age_test         207 non-null int64\n",
      "non_severe_hl    207 non-null float64\n",
      "mother_hs        134 non-null float64\n",
      "early_ident      207 non-null bool\n",
      "non_white        207 non-null bool\n",
      "dtypes: bool(3), float64(5), int64(3)\n",
      "memory usage: 15.2 KB\n"
     ]
    }
   ],
   "source": [
    "test_scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score             0\n",
       "male              0\n",
       "siblings          0\n",
       "family_inv       33\n",
       "non_english       0\n",
       "prev_disab       18\n",
       "age_test          0\n",
       "non_severe_hl     0\n",
       "mother_hs        73\n",
       "early_ident       0\n",
       "non_white         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.isnull().sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for dealing with missing data\n",
    "\n",
    "### ignore\n",
    "\n",
    "The easiest (and worst) way to deal with missing data is to **ignore it**. That is, simply run the analysis, missing values and all, hoping for the best. If your software is any good, this approach will simply not work; the algorithm will try to operate on data that includes missing values, and propagate them, resulting in statistics and estimates that cannot be calculated, which will typically raise errors. If your software is poor, it will make some assumption or decision about the missing values, and proceed to generate  results conditional on the assumption, which creates problems that may never be detected because no indication was given to any potential problem. \n",
    "\n",
    "### complete case analysis\n",
    "\n",
    "The next easiest (worst) approach to analyzing data with missing values is to conduct list-wise deletion, by deleting the records that have missing values. This is called **complete case analysis**, because only records that are complete get retained for the analysis. The degree to which complete case analysis is undesirable depends on the mechanism by which data have become missing.\n",
    "\n",
    "## Types of Missingness\n",
    "\n",
    "Think about Bias and Power\n",
    "\n",
    "- **Missing completely at random (MCAR)**: When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the model. Ignoring the missingness will reduce the power of the analysis, but will not bias inference.\n",
    "- **Missing at random (MAR)**: Missing data depends (usually probabilistically) on measured values, and hence can be modeled by variables observed in the data set. Accounting for the values which “cause” the missing data will produce unbiased results in an analysis.\n",
    "- **Missing not at random(MNAR)**: Missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness.\n",
    "\n",
    "The very best-case scenario for using complete case analysis, which corresponds to MCAR missingness, results in a **loss of power** due to the reduction in sample size. The analysis will lose the information contained in the non-missing elements of a partially-missing record. When data are not missing completely at random, inferences from complete case analysis may be **biased** due to systematic differences between missing and non-missing records that affects the estimates of key parameters.\n",
    "\n",
    "One alternative to complete case analysis is to simply fill (*impute*) the missing values with a reasonable guess a the true value, such as the mean, median or modal value of the fully-observed records. This imputation, while not recovering any information regarding the missing value itself for use in the analysis, does provide a mechanism for including the non-missing values in the analysis, thereby making use of all available information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Imputer` class in scikit-learn provides methods for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. This class also allows for different missing value encodings.\n",
    "\n",
    "For example, we can replace missing entries encoded as `np.nan` using the mean value of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  1.        ],\n",
       "       [ 6.        ,  3.66666667],\n",
       "       [ 3.        ,  6.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[np.nan, 1], [6, np.nan], [3, 6]]\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.,   0.,   2.,   2.,   0.,   0.,  55.,   1.,   1.,   0.,   0.],\n",
       "       [ 31.,   1.,   0.,   0.,   0.,   0.,  53.,   0.,   0.,   0.,   0.],\n",
       "       [ 83.,   1.,   1.,   1.,   1.,   0.,  52.,   1.,   1.,   0.,   1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use mode imputation for test_scores\n",
    "mode_imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "mode_imp.fit(test_scores)\n",
    "mode_imp.transform(test_scores)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some times it is easier to use the `fillna` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1256038647342994"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.siblings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "siblings_imputed = test_scores.siblings.fillna(test_scores.siblings.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach may be reasonable under the MCAR assumption, but may induce bias under a MAR scenario, whereby missing values may **differ systematically** relative to non-missing values, making the particular summary statistic used for imputation *biased* as a mean/median/modal value for the missing values.\n",
    "\n",
    "Beyond this, the use of a single imputed value to stand in place of the actual missing value glosses over the **uncertainty** associated with this guess at the true value. Any subsequent analysis procedure (*e.g.* regression analysis) will behave as if the imputed value were observed, despite the fact that we are actually unsure of the actual value for the missing variable. The practical consequence of this is that the variance of any estimates resulting from the imputed dataset will be **artificially reduced**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Imputation\n",
    "\n",
    "One robust alternative to addressing missing data is **multiple imputation** (Schaffer 1999, van Buuren 2012). It produces unbiased parameter estimates, while simultaneously accounting for the uncertainty associated with imputing missing values. It is conceptually and mechanistically straightforward, and produces complete datasets that may be analyzed using any statistical methodology or software one chooses, as if the data had no missing values to begin with.\n",
    "\n",
    "Multiple imputation generates imputed values based on a **regression model**. This regression model will help us generate reasonable values, particularly if data are MAR, since it uses information in the dataset that may be informative in predicting what the true value may be. Ideally, we want predictor variables that are **correlated** with the missing variable, and with the mechanism of missingness, if any. For example, one might be able to use test scores from one subject to predict missing test scores from another; or, the probability of income reporting to be missing may vary systematically according to the education level of the individual.\n",
    "\n",
    "To see if there is any potential information among the variables in our dataset to use for imputation, it is helpful to calculate the pairwise correlation between all the variables. Since we have discrete variables in our data, the [Spearman rank correlation coefficient](http://www.wikiwand.com/en/Spearman%27s_rank_correlation_coefficient) is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>family_inv</th>\n",
       "      <th>non_english</th>\n",
       "      <th>prev_disab</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073063</td>\n",
       "      <td>-0.085044</td>\n",
       "      <td>-0.539019</td>\n",
       "      <td>-0.278798</td>\n",
       "      <td>-0.184426</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>0.140305</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.222711</td>\n",
       "      <td>-0.345061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.073063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.081165</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>-0.048638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siblings</th>\n",
       "      <td>-0.085044</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>-0.049989</td>\n",
       "      <td>-0.038020</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>0.077318</td>\n",
       "      <td>0.006234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_inv</th>\n",
       "      <td>-0.539019</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221696</td>\n",
       "      <td>0.082314</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>-0.358898</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.401617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_english</th>\n",
       "      <td>-0.278798</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>-0.049989</td>\n",
       "      <td>0.221696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021996</td>\n",
       "      <td>0.068095</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>-0.199639</td>\n",
       "      <td>-0.015812</td>\n",
       "      <td>0.225428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_disab</th>\n",
       "      <td>-0.184426</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.038020</td>\n",
       "      <td>0.082314</td>\n",
       "      <td>-0.021996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>-0.021367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_test</th>\n",
       "      <td>0.024057</td>\n",
       "      <td>-0.081165</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>0.068095</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.068430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_severe_hl</th>\n",
       "      <td>0.140305</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.028480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_hs</th>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>-0.358898</td>\n",
       "      <td>-0.199639</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024411</td>\n",
       "      <td>-0.214209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_ident</th>\n",
       "      <td>0.222711</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.077318</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>-0.015812</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.024411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_white</th>\n",
       "      <td>-0.345061</td>\n",
       "      <td>-0.048638</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.401617</td>\n",
       "      <td>0.225428</td>\n",
       "      <td>-0.021367</td>\n",
       "      <td>0.068430</td>\n",
       "      <td>0.028480</td>\n",
       "      <td>-0.214209</td>\n",
       "      <td>-0.022854</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score      male  siblings  family_inv  non_english  \\\n",
       "score          1.000000  0.073063 -0.085044   -0.539019    -0.278798   \n",
       "male           0.073063  1.000000 -0.072006   -0.008714     0.053338   \n",
       "siblings      -0.085044 -0.072006  1.000000    0.078471    -0.049989   \n",
       "family_inv    -0.539019 -0.008714  0.078471    1.000000     0.221696   \n",
       "non_english   -0.278798  0.053338 -0.049989    0.221696     1.000000   \n",
       "prev_disab    -0.184426 -0.052054 -0.038020    0.082314    -0.021996   \n",
       "age_test       0.024057 -0.081165  0.104905   -0.029120     0.068095   \n",
       "non_severe_hl  0.140305  0.031825 -0.003689   -0.092815    -0.047775   \n",
       "mother_hs      0.228500  0.050372  0.096268   -0.358898    -0.199639   \n",
       "early_ident    0.222711 -0.007690  0.077318    0.006370    -0.015812   \n",
       "non_white     -0.345061 -0.048638  0.006234    0.401617     0.225428   \n",
       "\n",
       "               prev_disab  age_test  non_severe_hl  mother_hs  early_ident  \\\n",
       "score           -0.184426  0.024057       0.140305   0.228500     0.222711   \n",
       "male            -0.052054 -0.081165       0.031825   0.050372    -0.007690   \n",
       "siblings        -0.038020  0.104905      -0.003689   0.096268     0.077318   \n",
       "family_inv       0.082314 -0.029120      -0.092815  -0.358898     0.006370   \n",
       "non_english     -0.021996  0.068095      -0.047775  -0.199639    -0.015812   \n",
       "prev_disab       1.000000  0.136604       0.048132   0.137893     0.046592   \n",
       "age_test         0.136604  1.000000      -0.122811   0.016760     0.033789   \n",
       "non_severe_hl    0.048132 -0.122811       1.000000  -0.015996     0.008211   \n",
       "mother_hs        0.137893  0.016760      -0.015996   1.000000     0.024411   \n",
       "early_ident      0.046592  0.033789       0.008211   0.024411     1.000000   \n",
       "non_white       -0.021367  0.068430       0.028480  -0.214209    -0.022854   \n",
       "\n",
       "               non_white  \n",
       "score          -0.345061  \n",
       "male           -0.048638  \n",
       "siblings        0.006234  \n",
       "family_inv      0.401617  \n",
       "non_english     0.225428  \n",
       "prev_disab     -0.021367  \n",
       "age_test        0.068430  \n",
       "non_severe_hl   0.028480  \n",
       "mother_hs      -0.214209  \n",
       "early_ident    -0.022854  \n",
       "non_white       1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.dropna().corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to impute missing values the mother's high school education indicator variable, which takes values of 0 for no high school diploma, or 1 for high school diploma or greater. The appropriate model to predict binary variables is a **logistic regression**. We will use the scikit-learn implementation, `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will only use variables that are themselves complete to build the predictive model, hence our subset of predictors will exclude family involvement score (`family_inv`) and previous disability (`prev_disab`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impute_subset = test_scores.drop(labels=['family_inv','prev_disab','score'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we scale the predictor variables to range from 0 to 1, to improve the performance of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = impute_subset.pop('mother_hs').values\n",
    "X = preprocessing.StandardScaler().fit_transform(impute_subset.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a `LogisticRegression` model, and fit it using the non-missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = np.isnan(y)\n",
    "mod = LogisticRegression()\n",
    "mod.fit(X[~missing], y[~missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_hs_pred = mod.predict(X[missing])\n",
    "mother_hs_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can then be inserted in place of the missing values, and an analysis can be performed on the entire dataset.\n",
    "\n",
    "However, this is still just a single imputation for each missing value, and hence glosses over the uncertainty associated with the derivation of the imputes. Multiple imputation proceeds by **imputing several values**, to generate several complete datasets and performing the same analysis on all of them. With a set of estimates in hand, an *average* estimate of model parameters can be obtained that more adequately accounts for the uncertainty, hopefully providing more robust inference than from a single impute.\n",
    "\n",
    "There are a variety of ways to generate multiple imputations. Here, we will exploit **regularization** in order to do this. The `LogisticRegression` class from scikit-learn provides facilities for regularization using either L2 (resulting in ridge regression) or L1 (resulting in LASSO regression) penalties. The degree of regularization in either case is controlled by the `C` parameter, whereby large values of `C` give more freedom to the model, while smaller values of `C` constrain the model more. We can use a selection of `C` values to obtain a range of predictions from variants of the same model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2 = LogisticRegression(C=1, penalty='l1')\n",
    "mod2.fit(X[~missing], y[~missing])\n",
    "mod2.predict(X[missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3 = LogisticRegression(C=0.4, penalty='l1')\n",
    "mod3.fit(X[~missing], y[~missing])\n",
    "mod3.predict(X[missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly few imputations are required to acheive reasonable estimates, with 3-10 usually sufficient. We will use 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mother_hs_imp = []\n",
    "\n",
    "for C in 0.1, 0.4, 2:\n",
    "    \n",
    "    mod = LogisticRegression(C=C, penalty='l1')\n",
    "    mod.fit(X[~missing], y[~missing])\n",
    "    imputed = mod.predict(X[missing])\n",
    "    mother_hs_imp.append(imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
       " array([ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
       " array([ 1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_hs_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
